{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = [\n",
    "    [  # +\n",
    "        [5, -1, 0],\n",
    "        [2, 0, -5],\n",
    "        [1, 1, -5],\n",
    "    ],\n",
    "    [  # -\n",
    "        [-1, 2, 1],\n",
    "        [0, -0, 5],\n",
    "        [-5, 2, 3],\n",
    "    ],\n",
    "    [  # -\n",
    "        [-2, 0, 1],\n",
    "        [1, -4, 2],\n",
    "        [1, 2, 3],\n",
    "    ],\n",
    "    [  # -\n",
    "        [-3, 2, 1],\n",
    "        [3, -1, 3],\n",
    "        [-3, 2, 0],\n",
    "    ],\n",
    "    [  # -\n",
    "        [-4, 2, 1],\n",
    "        [2, -0, 2],\n",
    "        [-1, 2, 3],\n",
    "    ],\n",
    "    [  # -\n",
    "        [1, 2, 3],\n",
    "        [3, -5, 2],\n",
    "        [0, -2, 1],\n",
    "    ],\n",
    "\n",
    "]\n",
    "\n",
    "Ytrain = [\n",
    "    1,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest = [\n",
    "    [  # +\n",
    "        [-1, 1, 0],\n",
    "        [0, 1, 1],\n",
    "        [0, 1, 0],\n",
    "    ],\n",
    "    [  # -\n",
    "        [-2, 1, 1],\n",
    "        [-2, 1, 1],\n",
    "        [-5, 1, 1],\n",
    "    ],\n",
    "    [  # \\\n",
    "        [1, 1, 1],\n",
    "        [0, 1, 0],\n",
    "        [1, 1, 1],\n",
    "    ],\n",
    "]\n",
    "Stest = [\n",
    "    \"=|-\",\n",
    "    \"#\",\n",
    "    \"T\"\n",
    "]\n",
    "\n",
    "Ytest = [\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neuron Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Structure of Simple Liniar OOP Neuron ###########\n",
    "class LiniarNeuronDetect:\n",
    "    def __init__(self):\n",
    "        self.weights = [0 for i in range(9)]\n",
    "        self.bias = 0\n",
    "\n",
    "    def sigmoidActivation(self, x):\n",
    "        e = 2.718281828459045\n",
    "        return 1 / (1 + e ** -x)\n",
    "\n",
    "    def forward(self, X):\n",
    "        y = self.bias\n",
    "        for i in range(len(X)):\n",
    "            y += X[i] * self.weights[i]\n",
    "\n",
    "        y = self.sigmoidActivation(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "    def __str__(self):\n",
    "        out = \"\"\n",
    "        \n",
    "        for i in range(9):\n",
    "            if i % 3 == 0:\n",
    "                out += \"\\n\" \n",
    "            out += f\"{self.weights[i]:10.5f}\"\n",
    "        out += \"\\n\\n\"\n",
    "        out += f\"{self.bias:10.5f}\"\n",
    "        \n",
    "        return out\n",
    "        \n",
    "\n",
    "\n",
    "def loss(Y, Yp):\n",
    "    return Y- Yp\n",
    "\n",
    "\n",
    "\n",
    "def reshapeMatrix(X):\n",
    "    Xt = []\n",
    "    for i in range(len(X)):\n",
    "        for j in range(len(X)):\n",
    "            Xt.append(X[i][j])\n",
    "    # print(Xt)\n",
    "    return Xt\n",
    "\n",
    "# reshapeMatrix(Xtrain[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neuron Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "loss:    0.25551\n",
      "epoch: 2\n",
      "loss:    0.17812\n",
      "epoch: 3\n",
      "loss:    0.13355\n",
      "epoch: 4\n",
      "loss:    0.10569\n",
      "epoch: 5\n",
      "loss:    0.08701\n",
      "epoch: 6\n",
      "loss:    0.07374\n",
      "epoch: 7\n",
      "loss:    0.06389\n",
      "epoch: 8\n",
      "loss:    0.05629\n",
      "epoch: 9\n",
      "loss:    0.05028\n",
      "epoch: 10\n",
      "loss:    0.04541\n",
      "epoch: 11\n",
      "loss:    0.04138\n",
      "epoch: 12\n",
      "loss:    0.03800\n",
      "epoch: 13\n",
      "loss:    0.03512\n",
      "epoch: 14\n",
      "loss:    0.03264\n",
      "epoch: 15\n",
      "loss:    0.03049\n",
      "epoch: 16\n",
      "loss:    0.02860\n",
      "epoch: 17\n",
      "loss:    0.02692\n",
      "epoch: 18\n",
      "loss:    0.02543\n",
      "epoch: 19\n",
      "loss:    0.02410\n",
      "epoch: 20\n",
      "loss:    0.02290\n",
      "epoch: 21\n",
      "loss:    0.02181\n",
      "epoch: 22\n",
      "loss:    0.02082\n",
      "epoch: 23\n",
      "loss:    0.01991\n",
      "epoch: 24\n",
      "loss:    0.01908\n",
      "epoch: 25\n",
      "loss:    0.01832\n",
      "epoch: 26\n",
      "loss:    0.01761\n",
      "epoch: 27\n",
      "loss:    0.01696\n",
      "epoch: 28\n",
      "loss:    0.01635\n",
      "epoch: 29\n",
      "loss:    0.01578\n",
      "epoch: 30\n",
      "loss:    0.01526\n",
      "epoch: 31\n",
      "loss:    0.01476\n",
      "epoch: 32\n",
      "loss:    0.01430\n",
      "epoch: 33\n",
      "loss:    0.01386\n",
      "epoch: 34\n",
      "loss:    0.01346\n",
      "epoch: 35\n",
      "loss:    0.01307\n",
      "epoch: 36\n",
      "loss:    0.01270\n",
      "epoch: 37\n",
      "loss:    0.01236\n",
      "epoch: 38\n",
      "loss:    0.01203\n",
      "epoch: 39\n",
      "loss:    0.01172\n",
      "epoch: 40\n",
      "loss:    0.01143\n",
      "epoch: 41\n",
      "loss:    0.01115\n",
      "epoch: 42\n",
      "loss:    0.01088\n",
      "epoch: 43\n",
      "loss:    0.01063\n",
      "epoch: 44\n",
      "loss:    0.01038\n",
      "epoch: 45\n",
      "loss:    0.01015\n",
      "epoch: 46\n",
      "loss:    0.00993\n",
      "epoch: 47\n",
      "loss:    0.00972\n",
      "epoch: 48\n",
      "loss:    0.00952\n",
      "epoch: 49\n",
      "loss:    0.00932\n",
      "epoch: 50\n",
      "loss:    0.00913\n",
      "epoch: 51\n",
      "loss:    0.00895\n",
      "epoch: 52\n",
      "loss:    0.00878\n",
      "epoch: 53\n",
      "loss:    0.00861\n",
      "epoch: 54\n",
      "loss:    0.00845\n",
      "epoch: 55\n",
      "loss:    0.00830\n",
      "epoch: 56\n",
      "loss:    0.00815\n",
      "epoch: 57\n",
      "loss:    0.00801\n",
      "epoch: 58\n",
      "loss:    0.00787\n",
      "epoch: 59\n",
      "loss:    0.00773\n",
      "epoch: 60\n",
      "loss:    0.00760\n",
      "epoch: 61\n",
      "loss:    0.00748\n",
      "epoch: 62\n",
      "loss:    0.00736\n",
      "epoch: 63\n",
      "loss:    0.00724\n",
      "epoch: 64\n",
      "loss:    0.00713\n",
      "epoch: 65\n",
      "loss:    0.00702\n",
      "epoch: 66\n",
      "loss:    0.00691\n",
      "epoch: 67\n",
      "loss:    0.00681\n",
      "epoch: 68\n",
      "loss:    0.00671\n",
      "epoch: 69\n",
      "loss:    0.00661\n",
      "epoch: 70\n",
      "loss:    0.00651\n",
      "epoch: 71\n",
      "loss:    0.00642\n",
      "epoch: 72\n",
      "loss:    0.00633\n",
      "epoch: 73\n",
      "loss:    0.00624\n",
      "epoch: 74\n",
      "loss:    0.00616\n",
      "epoch: 75\n",
      "loss:    0.00608\n",
      "epoch: 76\n",
      "loss:    0.00600\n",
      "epoch: 77\n",
      "loss:    0.00592\n",
      "epoch: 78\n",
      "loss:    0.00584\n",
      "epoch: 79\n",
      "loss:    0.00577\n",
      "epoch: 80\n",
      "loss:    0.00570\n",
      "epoch: 81\n",
      "loss:    0.00563\n",
      "epoch: 82\n",
      "loss:    0.00556\n",
      "epoch: 83\n",
      "loss:    0.00549\n",
      "epoch: 84\n",
      "loss:    0.00542\n",
      "epoch: 85\n",
      "loss:    0.00536\n",
      "epoch: 86\n",
      "loss:    0.00530\n",
      "epoch: 87\n",
      "loss:    0.00524\n",
      "epoch: 88\n",
      "loss:    0.00518\n",
      "epoch: 89\n",
      "loss:    0.00512\n",
      "epoch: 90\n",
      "loss:    0.00506\n",
      "epoch: 91\n",
      "loss:    0.00500\n",
      "epoch: 92\n",
      "loss:    0.00495\n",
      "epoch: 93\n",
      "loss:    0.00490\n",
      "epoch: 94\n",
      "loss:    0.00484\n",
      "epoch: 95\n",
      "loss:    0.00479\n",
      "epoch: 96\n",
      "loss:    0.00474\n",
      "epoch: 97\n",
      "loss:    0.00469\n",
      "epoch: 98\n",
      "loss:    0.00465\n",
      "epoch: 99\n",
      "loss:    0.00460\n",
      "epoch: 100\n",
      "loss:    0.00455\n",
      "epoch: 101\n",
      "loss:    0.00451\n",
      "epoch: 102\n",
      "loss:    0.00446\n",
      "epoch: 103\n",
      "loss:    0.00442\n",
      "epoch: 104\n",
      "loss:    0.00438\n",
      "epoch: 105\n",
      "loss:    0.00434\n",
      "epoch: 106\n",
      "loss:    0.00429\n",
      "epoch: 107\n",
      "loss:    0.00425\n",
      "epoch: 108\n",
      "loss:    0.00421\n",
      "epoch: 109\n",
      "loss:    0.00418\n",
      "epoch: 110\n",
      "loss:    0.00414\n",
      "epoch: 111\n",
      "loss:    0.00410\n",
      "epoch: 112\n",
      "loss:    0.00406\n",
      "epoch: 113\n",
      "loss:    0.00403\n",
      "epoch: 114\n",
      "loss:    0.00399\n",
      "epoch: 115\n",
      "loss:    0.00396\n",
      "epoch: 116\n",
      "loss:    0.00392\n",
      "epoch: 117\n",
      "loss:    0.00389\n",
      "epoch: 118\n",
      "loss:    0.00386\n",
      "epoch: 119\n",
      "loss:    0.00382\n",
      "epoch: 120\n",
      "loss:    0.00379\n",
      "epoch: 121\n",
      "loss:    0.00376\n",
      "epoch: 122\n",
      "loss:    0.00373\n",
      "epoch: 123\n",
      "loss:    0.00370\n",
      "epoch: 124\n",
      "loss:    0.00367\n",
      "epoch: 125\n",
      "loss:    0.00364\n",
      "epoch: 126\n",
      "loss:    0.00361\n",
      "epoch: 127\n",
      "loss:    0.00358\n",
      "epoch: 128\n",
      "loss:    0.00355\n",
      "epoch: 129\n",
      "loss:    0.00353\n",
      "epoch: 130\n",
      "loss:    0.00350\n",
      "epoch: 131\n",
      "loss:    0.00347\n",
      "epoch: 132\n",
      "loss:    0.00345\n",
      "epoch: 133\n",
      "loss:    0.00342\n",
      "epoch: 134\n",
      "loss:    0.00339\n",
      "epoch: 135\n",
      "loss:    0.00337\n",
      "epoch: 136\n",
      "loss:    0.00334\n",
      "epoch: 137\n",
      "loss:    0.00332\n",
      "epoch: 138\n",
      "loss:    0.00330\n",
      "epoch: 139\n",
      "loss:    0.00327\n",
      "epoch: 140\n",
      "loss:    0.00325\n",
      "epoch: 141\n",
      "loss:    0.00323\n",
      "epoch: 142\n",
      "loss:    0.00320\n",
      "epoch: 143\n",
      "loss:    0.00318\n",
      "epoch: 144\n",
      "loss:    0.00316\n",
      "epoch: 145\n",
      "loss:    0.00314\n",
      "epoch: 146\n",
      "loss:    0.00311\n",
      "epoch: 147\n",
      "loss:    0.00309\n",
      "epoch: 148\n",
      "loss:    0.00307\n",
      "epoch: 149\n",
      "loss:    0.00305\n",
      "epoch: 150\n",
      "loss:    0.00303\n",
      "epoch: 151\n",
      "loss:    0.00301\n",
      "epoch: 152\n",
      "loss:    0.00299\n",
      "epoch: 153\n",
      "loss:    0.00297\n",
      "epoch: 154\n",
      "loss:    0.00295\n",
      "epoch: 155\n",
      "loss:    0.00293\n",
      "epoch: 156\n",
      "loss:    0.00291\n",
      "epoch: 157\n",
      "loss:    0.00290\n",
      "epoch: 158\n",
      "loss:    0.00288\n",
      "epoch: 159\n",
      "loss:    0.00286\n",
      "epoch: 160\n",
      "loss:    0.00284\n",
      "epoch: 161\n",
      "loss:    0.00282\n",
      "epoch: 162\n",
      "loss:    0.00281\n",
      "epoch: 163\n",
      "loss:    0.00279\n",
 .............................
 .............................
      "epoch: 1982\n",
      "loss:    0.00023\n",
      "epoch: 1983\n",
      "loss:    0.00023\n",
      "epoch: 1984\n",
      "loss:    0.00023\n",
      "epoch: 1985\n",
      "loss:    0.00023\n",
      "epoch: 1986\n",
      "loss:    0.00023\n",
      "epoch: 1987\n",
      "loss:    0.00023\n",
      "epoch: 1988\n",
      "loss:    0.00023\n",
      "epoch: 1989\n",
      "loss:    0.00023\n",
      "epoch: 1990\n",
      "loss:    0.00023\n",
      "epoch: 1991\n",
      "loss:    0.00023\n",
      "epoch: 1992\n",
      "loss:    0.00023\n",
      "epoch: 1993\n",
      "loss:    0.00023\n",
      "epoch: 1994\n",
      "loss:    0.00023\n",
      "epoch: 1995\n",
      "loss:    0.00023\n",
      "epoch: 1996\n",
      "loss:    0.00023\n",
      "epoch: 1997\n",
      "loss:    0.00023\n",
      "epoch: 1998\n",
      "loss:    0.00023\n",
      "epoch: 1999\n",
      "loss:    0.00023\n"
     ]
    }
   ],
   "source": [
    "neuron = LiniarNeuronDetect()\n",
    "\n",
    "\n",
    "lr = 0.01\n",
    "\n",
    "for epoch in range(1,2000):  \n",
    "    errors = [] \n",
    "    for i in range (len(Xtrain)):\n",
    "        # FORWARD PASS\n",
    "        xt = Xtrain[i]\n",
    "        yt = Ytrain[i]\n",
    "        xt = reshapeMatrix(xt)\n",
    "        yp = neuron.forward(xt)\n",
    "\n",
    "        # ESTIMATE LOSS\n",
    "        e = loss(yt,yp)\n",
    "        errors.append(e)\n",
    "        # BACKPROPAGATION\n",
    "        dBias = lr * e \n",
    "        neuron.bias += dBias\n",
    "\n",
    "        for j in range(len(xt)):\n",
    "            dWeight = lr * e * xt[j]\n",
    "            neuron.weights[j] += dWeight \n",
    "\n",
    "    me = abs(sum(errors) / len(errors))  \n",
    "\n",
    "    print(f\"epoch: {epoch}\")\n",
    "    print(f\"loss: {me:10.5f}\")\n",
    "    # print(neuron)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neuron Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:    0.00015  Yp:    0.99985\n",
      "loss:   -0.00011  Yp:    0.00011\n",
      "loss:   -0.00039  Yp:    0.00039\n",
      "loss:   -0.00027  Yp:    0.00027\n",
      "loss:   -0.00028  Yp:    0.00028\n",
      "loss:   -0.00047  Yp:    0.00047\n",
      "loss:   -0.19383  Yp:    0.19383\n",
      "loss:   -0.02990  Yp:    0.02990\n",
      "loss:   -0.41984  Yp:    0.41984\n"
     ]
    }
   ],
   "source": [
    "X = Xtrain + Xtest\n",
    "Y = Ytrain + Ytest\n",
    "# S = Strain + Stest\n",
    "for i in  range(len(X)):\n",
    "    xt = X[i]\n",
    "    yt = Y[i]\n",
    "    xt = reshapeMatrix(xt)\n",
    "    yp = neuron.forward(xt)\n",
    "    e = loss(yt,yp)\n",
    "    print(f\"loss: {e: 10.5f}  Yp: {yp:10.5f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "90528ac9f84e6e59cadb7d09fb5449f76510f62cd5a4c7477374219f96764f55"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
